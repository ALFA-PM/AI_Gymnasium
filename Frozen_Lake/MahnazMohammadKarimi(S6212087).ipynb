{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb86c53c",
   "metadata": {},
   "source": [
    "# Frozen Lake Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5785c3",
   "metadata": {},
   "source": [
    "The Frozen Lake problem in reinforcement learning involves navigating a grid-like environment represented as a frozen lake with holes. The goal is to reach a target location without falling into the holes. The q-Learning method is applied to learn an optimal strategy: it iteratively updates a Q-table, which estimates the value of taking certain actions in specific states. This table guides the agent to make decisions that maximize long-term rewards. The agent learns through exploration (trying random actions) and exploitation (using the best-known action), gradually refining its policy to navigate the lake successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722fd71",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef96bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium[toy-text]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium[toy-text]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium[toy-text]) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium[toy-text]) (2.5.2)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[toy-text]\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6eb09",
   "metadata": {},
   "source": [
    "# Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494334e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9862e6",
   "metadata": {},
   "source": [
    "# epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8aaaa",
   "metadata": {},
   "source": [
    "The function `epsilon_greedy_policy` selects an action for a given state in a reinforcement learning environment using an epsilon-greedy approach. It first determines the number of possible actions `num_actions` in the current state from the Q-table, which contains the expected rewards for each action at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff31b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q_table, state, epsilon):\n",
    "    \"\"\"\n",
    "    Selects an action using epsilon-greedy policy\n",
    "\n",
    "    Args:\n",
    "    Q_table (numpy.ndarray): Q-value table\n",
    "    state (int): Current state\n",
    "    epsilon (float): Epsilon value for exploration-exploitation trade-off\n",
    "\n",
    "    Returns:\n",
    "    int: Selected action\n",
    "    \"\"\"\n",
    "    # Determine the number of actions available in the current state\n",
    "    num_actions = len(Q_table[state])\n",
    "\n",
    "    # Decide whether to explore or exploit\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        # Explore: choose a random action\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        # Exploit: choose the best action based on current Q-values\n",
    "        return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106d05a",
   "metadata": {},
   "source": [
    "# q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4747fd0",
   "metadata": {},
   "source": [
    "The `q_learning` function implements the q-Learning algorithm for a given environment over a specified number of episodes. It initializes a Q-table to store action values and iteratively updates it based on the temporal difference target, balancing between exploration and exploitation using the epsilon-greedy policy. The learning rate `alpha` and discount factor `gamma` influence how the Q-table is updated. Progress is monitored using a progress bar, and the policy's performance is evaluated intermittently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b42e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, episodes, alpha=0.2, gamma=0.99, epsilon=0.1):\n",
    "    Q_table = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "    pbar = tqdm(total=episodes, dynamic_ncols=True)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = environment.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(Q_table, state, epsilon)\n",
    "            next_state, reward, done, _, _ = environment.step(action)\n",
    "            next_action = np.argmax(Q_table[next_state, :])\n",
    "\n",
    "            td_target = reward + gamma * Q_table[next_state, next_action]\n",
    "            Q_table[state, action] += alpha * (td_target - Q_table[state, action])\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Evaluate policy less frequently to save computation\n",
    "        if episode % 1000 == 0 or episode == episodes - 1:\n",
    "            avg_reward = evaluate_policy(environment, Q_table, 100)\n",
    "            pbar.set_description(f\"Episode: {episode} - Average Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    pbar.close()\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b12b9",
   "metadata": {},
   "source": [
    "# evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfb8ab",
   "metadata": {},
   "source": [
    "The `evaluate_policy` function assesses the effectiveness of a policy, derived from a Q-table, in a specified environment over a set number of episodes. It calculates the policy by taking the action with the highest value in the Q-table for each state. For each episode, the function resets the environment, then continually selects actions according to the policy and updates the state based on the environment's response, accumulating rewards until the episode ends. The function finally returns the average total reward per episode, providing a measure of the policy's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0855a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(environment, Q_table, episodes):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a given policy over a certain number of episodes\n",
    "\n",
    "    Args:\n",
    "    environment: The environment to test the policy in\n",
    "    Q_table (numpy.ndarray): The Q-table representing the policy\n",
    "    episodes (int): Number of episodes to run the evaluation for\n",
    "\n",
    "    Returns:\n",
    "    float: The average total reward per episode\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    # Extract the policy from the Q-table\n",
    "    optimal_policy = np.argmax(Q_table, axis=1)\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state, _ = environment.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = optimal_policy[state]\n",
    "            state, reward, done, _, _ = environment.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    # Calculate the average reward per episode\n",
    "    average_reward = total_reward / episodes\n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d059a7c",
   "metadata": {},
   "source": [
    "The `demo_agent` function visually demonstrates the behavior of an agent in a specified environment using a policy derived from a Q-table. It first computes the optimal policy by selecting the action with the highest value in the Q-table for each state. For each requested episode, the function resets the environment and then repeatedly chooses actions according to the optimal policy, updating the state based on the environment's response. If `render` is set to `True` , the environment's state is visually rendered at each step, allowing for a visual representation of the agent's actions and decisions. The function continues until the episode ends, showcasing how the agent navigates the environment using the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29dff8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_agent(environment, Q_table, episodes=1, render=True):\n",
    "    \"\"\"\n",
    "    Demonstrates the behavior of an agent in a given environment using the policy derived from the Q-table\n",
    "\n",
    "    Args:\n",
    "    environment: The environment in which to demonstrate the agent\n",
    "    Q_table (numpy.ndarray): The Q-table used to derive the policy\n",
    "    episodes (int): Number of episodes to demonstrate\n",
    "    render (bool): If True, the environment will be rendered during the demonstration\n",
    "    \"\"\"\n",
    "    optimal_policy = np.argmax(Q_table, axis=1)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = environment.reset()\n",
    "        done = False\n",
    "        print(\"\\nEpisode:\", episode + 1)\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                environment.render()\n",
    "            action = optimal_policy[state]\n",
    "            state, _, done, _, _ = environment.step(action)\n",
    "\n",
    "        if render:\n",
    "            environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8e667",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59957ebe",
   "metadata": {},
   "source": [
    "The `main` function orchestrates the process of training an agent using q-Learning, evaluating its performance, and demonstrating the learned behavior in the FrozenLake environment. Initially, it sets up the environment and runs the q-Learning algorithm for a specified number of episodes, resulting in a trained Q-table. It then evaluates the effectiveness of the learned policy by calculating the average reward over the same number of episodes. This performance metric is printed out. After evaluation, the function demonstrates the agent's behavior in a visually-renderable version of the environment for a set number of demo episodes. Error handling is included to catch and report exceptions, ensuring that the environments are properly closed regardless of whether the process completes successfully or encounters an error. This structured approach helps in understanding the complete lifecycle of an agent's training and deployment in a reinforcement learning setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2126d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(episodes=10000, demo_episodes=5):\n",
    "    \"\"\"\n",
    "    Main function to run Q-Learning on the FrozenLake environment\n",
    "\n",
    "    Args:\n",
    "    episodes (int): Number of episodes to run Q-Learning\n",
    "    demo_episodes (int): Number of episodes to demonstrate the learned policy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create and run Q-learning on the environment\n",
    "        environment = gym.make(\"FrozenLake-v1\")\n",
    "        Q_table = q_learning(environment, episodes)\n",
    "\n",
    "        # Evaluate the learned policy\n",
    "        avg_reward = evaluate_policy(environment, Q_table, episodes)\n",
    "        print(f\"Average reward after q-learning: {avg_reward}\")\n",
    "\n",
    "        # Demonstrate the learned policy\n",
    "        visual_env = gym.make('FrozenLake-v1', render_mode='human')\n",
    "        demo_agent(visual_env, Q_table, demo_episodes)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up and close the environment\n",
    "        environment.close()\n",
    "        visual_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1beb780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 9999 - Average Reward: 0.80: 100%|████████████████████████████████████| 10000/10000 [00:21<00:00, 460.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward after q-learning: 0.8249\n",
      "\n",
      "Episode: 1\n",
      "\n",
      "Episode: 2\n",
      "\n",
      "Episode: 3\n",
      "\n",
      "Episode: 4\n",
      "\n",
      "Episode: 5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54388e98",
   "metadata": {},
   "source": [
    "The `demo_episodes` is 5, so it runs 5 times and then close. If it occures any bug, please stop the code and run the code again!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
